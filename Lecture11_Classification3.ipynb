{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de7f9edb",
   "metadata": {},
   "source": [
    "# Lecture 11: Classification3 Part 2\n",
    "Example for na誰ve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c24a2",
   "metadata": {},
   "source": [
    "### Example\n",
    "The example we are going to follow for the na誰ve Bayes classifier is based on text classification. Instead of a spam detector, we will work with Twitter data and check whether a tweet is related to data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd7c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('Train_QuantumTunnel_Tweets.csv', encoding='utf-8')\n",
    "# In Python 3 it is better to specify the encoding of a text file. In this case UTF-8.\n",
    "\n",
    "print(train[62:65])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2867c8c2",
   "metadata": {},
   "source": [
    "We would like to pre-process the text of the tweets to get rid of URLs and hashtags. \n",
    "\n",
    "Read about [Regular Expressions](https://www.w3schools.com/python/python_regex.asp), [Cheat Sheet](https://www.rexegg.com/regex-quickstart.html), [Simple Example](https://www.w3schools.com/python/trypython.asp?filename=demo_regex_match_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7126994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regular expressions package\n",
    "def tw_preprocess(tw):\n",
    "    ptw = re.sub(r\"http\\S+\", \"\", tw)\n",
    "    ptw = re.sub(r\"#\", \"\", ptw)\n",
    "    return ptw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Tweet'] = train['Tweet'].apply(tw_preprocess)\n",
    "\n",
    "print(train[62:65])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a3423",
   "metadata": {},
   "source": [
    "Generate a term-document matrix is a matrix whose rows correspond to documents and its columns to words. We can get\n",
    "this done with the help of *CountVectorizer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectoriser = CountVectorizer(lowercase=True,stop_words='english',binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ef7e2e",
   "metadata": {},
   "source": [
    "We can now apply our vectoriser to the training tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad1f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectoriser.fit_transform(train['Tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6778e8f",
   "metadata": {},
   "source": [
    "The result is a large sparse matrix, so printing it is not a good idea. Nonetheless, we can still see the vocabulary that has been gathered from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbaaafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser.get_feature_names_out()[1005:1011]  \n",
    "# We can list the vocabulary created with the help of get_feature_names_out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c8090",
   "metadata": {},
   "source": [
    "We are now in a position to create our model using the sparse matrix generated by our vectoriser and the labels provided with the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f531493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "model = naive_bayes.MultinomialNB().fit(X_train, list(train['Data_Science']))\n",
    "# We are using the MultinomialNB algorithm from naive_bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3516a2b6",
   "metadata": {},
   "source": [
    "Confusion matrix on the training set, which should be very nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(train['Data_Science'],model.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f23e709",
   "metadata": {},
   "source": [
    "Finally, we can now apply our model to the testing dataset provided. We need to load the data and let us apply the same preprocessing we used for the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Test_QuantumTunnel_Tweets.csv',encoding='utf-8')\n",
    "test['Tweet'] = test['Tweet'].apply(tw_preprocess)\n",
    "\n",
    "X_test = vectoriser.transform(test['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6344f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "print(pred)\n",
    "\n",
    "pred_probs = model.predict_proba(X_test)[:,1]\n",
    "print(pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9a6668",
   "metadata": {},
   "source": [
    "Let us check for example the probability assigned to the tweet with id 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb48997",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs[102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857e8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[102:103])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9f7c9",
   "metadata": {},
   "source": [
    "The rest of the data can be checked in a similar fashion. Notice that we have used a very small corpus for this demo, but you can see how powerful na誰ve Bayes is, even when it is na誰ve. Furthermore, in this example we were not particularly careful when cleaning the data, for instance we left numbers and punctuation in the corpus, also we did not apply any stemming on the text either."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
